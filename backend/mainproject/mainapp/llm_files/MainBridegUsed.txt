from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
import time
import re
import logging
import gc

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# -------------------------------------------------------------
# ‚úÖ Memory Management Functions
# -------------------------------------------------------------
def cleanup_memory():
    """Force cleanup of GPU and CPU memory"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    logger.info("üßπ Memory cleanup completed")

def safe_tensor_delete(tensors):
    """Safely delete tensors and free memory"""
    for name, tensor in list(tensors.items()):
        if tensor is not None:
            del tensor
    cleanup_memory()

# -------------------------------------------------------------
# ‚úÖ Load model (optimized for RTX 4050 - 8GB VRAM)
# -------------------------------------------------------------
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"

print("üöÄ Loading Mistral model for RTX 4050... Please wait...")

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # Add padding token if it doesn't exist
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Optimized model loading with memory management
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )

    # Set model to evaluation mode
    model.eval()
    
    device = next(model.parameters()).device
    print(f"‚úÖ Mistral model loaded successfully on: {device}")

except Exception as e:
    print(f"‚ùå Error loading model: {e}")
    model, tokenizer = None, None

def extract_operations_from_response(response_text):
    """Extract preprocessing operations from AI response - FAST VERSION"""
    print("üîç Extracting operations from AI response...")

    # FAST METHOD: Direct JSON parsing only
    json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
    if json_match:
        try:
            json_str = json_match.group()
            # Clean the JSON string
            json_str = re.sub(r'[\n\t]', ' ', json_str)
            operations = json.loads(json_str)
            if isinstance(operations, list) and len(operations) > 0:
                print(f"‚úÖ Found JSON operations: {operations}")
                return operations
        except json.JSONDecodeError as e:
            print(f"‚ùå JSON parsing failed: {e}")

    # NO PATTERN MATCHING - Return empty for speed
    print("‚ùå No valid JSON operations found")
    return []

def extract_column_operations_from_response(response_text):
    """Extract column-wise operations - FAST VERSION"""
    print("üîç Extracting column-wise operations...")

    try:
        # Look for JSON object only
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            json_str = re.sub(r'[\n\t]', ' ', json_str)
            operations_dict = json.loads(json_str)

            if isinstance(operations_dict, dict) and len(operations_dict) > 0:
                print(f"‚úÖ Found column-wise operations")
                return operations_dict
    except json.JSONDecodeError as e:
        print(f"‚ùå JSON parsing failed: {e}")

    return {}

# -------------------------------------------------------------
# ‚úÖ Health check
# -------------------------------------------------------------
@app.route("/health", methods=["GET"])
def health():
    return jsonify({
        "status": "ok",
        "model_loaded": model is not None,
        "service": "Mistral AI Preprocessing Advisor",
        "device": str(next(model.parameters()).device) if model else "unknown",
        "memory_optimized": True
    }), 200

# -------------------------------------------------------------
# ‚úÖ AI Analysis Route - OPTIMIZED FOR SPEED & MEMORY
# -------------------------------------------------------------
@app.route("/analyze", methods=["POST"])
def analyze_dataset():
    """Accept dataset summary and return AI preprocessing suggestions"""
    if model is None:
        return jsonify({"error": "Model not loaded properly"}), 500

    start_time = time.time()

    try:
        # -----------------------------
        # üì• Get data from request
        # -----------------------------
        data = request.get_json(force=True)
        summary = data.get("summary", "").strip()
        analysis_type = data.get("analysis_type", "fast")  # DEFAULT TO FAST

        if not summary:
            return jsonify({"error": "Missing 'summary' field"}), 400

        print("üì• Received dataset summary for analysis...")
        print(f"Summary preview: {summary[:200]}...")
        print(f"Analysis type: {analysis_type}")

        if analysis_type == "column_wise":
            result = _analyze_column_wise(summary, start_time)
        elif analysis_type == "fast":
            result = _analyze_fast(summary, start_time)
        else:
            result = _analyze_basic(summary, start_time)
        
        # Force memory cleanup after each request
        cleanup_memory()
        return result

    except Exception as e:
        logger.error(f"‚ùå Error in /analyze: {str(e)}")
        cleanup_memory()  # Cleanup even on error
        return jsonify({"success": False, "error": str(e)}), 500

def _analyze_fast(summary, start_time):
    """ULTRA-FAST analysis - 30-60 seconds max with memory management"""
    try:
        # -----------------------------
        # üß† FAST AI prompt - minimal operations
        # -----------------------------
        prompt = f"""<s>[INST] You are a data preprocessing expert. Analyze this dataset summary and recommend the 3 most essential preprocessing steps.

Available operations:
- remove:duplicates
- validate:datatypes
- impute:auto
- encode:categorical
- scale:standard
- handle:outliers

Dataset Summary:
{summary}

Respond ONLY with a JSON array of exactly 3 operations. Example: ["remove:duplicates", "impute:auto", "encode:categorical"]

JSON: [/INST]"""

        # -----------------------------
        # ‚öôÔ∏è Generate with model - ULTRA FAST PARAMETERS
        # -----------------------------
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=80,  # Very short response
                temperature=0.3,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1,
                num_return_sequences=1
            )

        # Extract response
        generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
        ai_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

        print("‚úÖ AI model responded successfully!")
        print(f"üß† Raw AI output: {ai_response}")
        
        # Extract operations
        operations = extract_operations_from_response(ai_response)
        elapsed = round(time.time() - start_time, 2)

        # üßπ CRITICAL: Clean up tensors immediately
        safe_tensor_delete({'inputs': inputs, 'outputs': outputs, 'generated_tokens': generated_tokens})

        print("=" * 50)
        print("üìä FAST ANALYSIS COMPLETED")
        print(f"‚úÖ SUGGESTED OPERATIONS: {operations}")
        print(f"‚ö° Generation time: {elapsed} seconds\n")

        return jsonify({
            "success": True,
            "generation_time": elapsed,
            "steps": operations,
            "ai_response_raw": ai_response[:200],
            "model_used": MODEL_NAME,
            "analysis_type": "fast",
            "memory_managed": True
        }), 200

    except Exception as e:
        cleanup_memory()
        raise e

def _analyze_basic(summary, start_time):
    """Basic analysis - 1-2 minutes max with memory management"""
    try:
        prompt = f"""<s>[INST] You are a data preprocessing expert. Analyze this dataset summary and recommend preprocessing steps.

Available operations:
- remove:duplicates
- validate:datatypes
- impute:auto
- encode:categorical
- scale:standard
- handle:outliers

Dataset Summary:
{summary}

Respond ONLY with a JSON array of operations. Example: ["remove:duplicates", "impute:auto", "encode:categorical"]

JSON: [/INST]"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1536).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=120,
                temperature=0.5,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1,
                num_return_sequences=1
            )

        generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
        ai_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

        print("‚úÖ AI model responded successfully!")
        operations = extract_operations_from_response(ai_response)
        elapsed = round(time.time() - start_time, 2)

        # üßπ Clean up tensors
        safe_tensor_delete({'inputs': inputs, 'outputs': outputs, 'generated_tokens': generated_tokens})

        print("=" * 50)
        print("üìä BASIC ANALYSIS COMPLETED")
        print(f"‚úÖ SUGGESTED OPERATIONS: {operations}")
        print(f"‚è±Ô∏è Generation time: {elapsed} seconds\n")

        return jsonify({
            "success": True,
            "generation_time": elapsed,
            "steps": operations,
            "ai_response_raw": ai_response[:300],
            "model_used": MODEL_NAME,
            "analysis_type": "basic",
            "memory_managed": True
        }), 200

    except Exception as e:
        cleanup_memory()
        raise e

def _analyze_column_wise(summary, start_time):
    """Column-wise analysis - Only when specifically requested"""
    try:
        prompt = f"""<s>[INST] You are a data preprocessing expert. Analyze this dataset and recommend column operations.
Available operations:
Numerical: impute:mean, impute:median, scale:standard, scale:minmax
Categorical: impute:mode, encode:onehot, encode:label
Dataset: remove:duplicates, validate:datatypes

Dataset Summary:
{summary}

Respond with JSON object. Example:
{{"Age": ["impute:median"], "Gender": ["encode:onehot"], "dataset_wide": ["remove:duplicates"]}}

JSON: [/INST]"""

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.3,
                top_p=0.85,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.05,
                num_return_sequences=1
            )

        generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]
        ai_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

        print("‚úÖ AI model responded successfully!")
        column_operations = extract_column_operations_from_response(ai_response)
        elapsed = round(time.time() - start_time, 2)

        # üßπ Clean up tensors
        safe_tensor_delete({'inputs': inputs, 'outputs': outputs, 'generated_tokens': generated_tokens})

        print("=" * 60)
        print("üìä COLUMN-WISE ANALYSIS COMPLETED")
        for column, operations in column_operations.items():
            if column == "dataset_wide":
                print(f"üåê DATASET-WIDE: {operations}")
            else:
                print(f"  üìç {column}: {operations}")
        print(f"‚è±Ô∏è Generation time: {elapsed} seconds\n")

        return jsonify({
            "success": True,
            "generation_time": elapsed,
            "column_operations": column_operations,
            "steps": _flatten_operations(column_operations),
            "ai_response_raw": ai_response[:400],
            "model_used": MODEL_NAME,
            "analysis_type": "column_wise",
            "memory_managed": True
        }), 200

    except Exception as e:
        cleanup_memory()
        raise e

def _flatten_operations(column_operations):
    """Flatten column operations"""
    all_operations = []
    for column, operations in column_operations.items():
        all_operations.extend(operations)
    return list(set(all_operations))

# -------------------------------------------------------------
# ‚úÖ Test endpoints with memory management
# -------------------------------------------------------------
@app.route("/test_fast", methods=["GET"])
def test_fast():
    """Test fast analysis"""
    if model is None:
        return jsonify({"error": "Model not loaded"}), 500

    try:
        test_prompt = """<s>[INST] Recommend 3 preprocessing steps for a dataset with missing values and categorical variables. JSON array only. [/INST]"""

        inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=60,
                temperature=0.3,
                do_sample=True
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Clean up
        safe_tensor_delete({'inputs': inputs, 'outputs': outputs})
        
        return jsonify({
            "test_response": response,
            "model_status": "working",
            "analysis_type": "fast",
            "memory_managed": True
        })

    except Exception as e:
        cleanup_memory()
        return jsonify({"error": str(e)}), 500

# -------------------------------------------------------------
# ‚úÖ Run Flask Server
# -------------------------------------------------------------
if __name__ == "__main__":
    print("üî• Starting OPTIMIZED Mistral Flask API on http://127.0.0.1:5005")
    print("üí° ULTRA-FAST mode enabled by default")
    print("üßπ MEMORY MANAGEMENT: Active (no memory leaks)")
    print("üìö Available endpoints:")
    print("   GET  /health        - Check server status")
    print("   POST /analyze       - Dataset analysis (fast by default)")
    print("   GET  /test_fast     - Test fast analysis")
    app.run(host="127.0.0.1", port=5005, debug=False, threaded=True)